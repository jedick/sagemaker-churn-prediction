{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "461b20a7-74df-4036-bf1a-0fe6b80cd371",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Run this code to preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd59705-5320-4d07-9ec1-9989b22ccb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset\n",
    "    \"\"\"\n",
    "    # Convert to datetime columns\n",
    "    df[\"firstorder\"] = pd.to_datetime(df[\"firstorder\"], errors=\"coerce\")\n",
    "    df[\"lastorder\"] = pd.to_datetime(df[\"lastorder\"], errors=\"coerce\")\n",
    "    # Drop Rows with null values\n",
    "    df = df.dropna()\n",
    "    # Create column which gives the days between the first and last orders\n",
    "    df[\"first_last_days_diff\"] = (df[\"lastorder\"] - df[\"firstorder\"]).dt.days\n",
    "    # Create column which gives the days between creation and first order\n",
    "    df[\"created\"] = pd.to_datetime(df[\"created\"])\n",
    "    df[\"created_first_days_diff\"] = (df[\"created\"] - df[\"firstorder\"]).dt.days\n",
    "    # Drop unused columns\n",
    "    unused_columns = [\"custid\", \"created\", \"firstorder\", \"lastorder\"]\n",
    "    df.drop(unused_columns, axis=1, inplace=True)\n",
    "    # Apply one hot encoding on categorical columns\n",
    "    cat_columns = ['favday', 'city']\n",
    "    df = pd.get_dummies(df, prefix=cat_columns, columns=cat_columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Define the S3 bucket and file key\n",
    "bucket = \"churn-prediction-sagemaker-demo\"\n",
    "file_key = \"data/storedata_total.csv\"\n",
    "# Create an S3 client\n",
    "s3_client = boto3.client(\"s3\")\n",
    "# Get the object from S3\n",
    "obj = s3_client.get_object(Bucket=bucket, Key=file_key)\n",
    "# Read the object content and load it into a pandas DataFrame\n",
    "df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "# Preprocess the dataset\n",
    "storedata = preprocess_data(df)\n",
    "storedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e567604c-0441-4987-b9c7-34c44573f34f",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "Run this code to split the data and upload the splits to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c469b-49a3-401a-988b-84e55e5219b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "def split_dataset(df):\n",
    "    y = df.pop(\"retained\")\n",
    "    X_pre = df\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "    feature_names = list(X_pre.columns)\n",
    "    X = np.concatenate((y_pre, X_pre), axis=1)\n",
    "    np.random.shuffle(X)\n",
    "    train, validation, test = np.split(X, [int(.7*len(X)), int(.85*len(X))])\n",
    "    return feature_names, train, validation, test\n",
    "\n",
    "\n",
    "# Copy the dataset for easier debugging\n",
    "df = storedata.copy()\n",
    "# Split dataset\n",
    "feature_names, train, validation, test = split_dataset(df)\n",
    "\n",
    "# Save datasets in Amazon S3\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "pd.DataFrame(train).to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3_resource.Object(bucket, \"data/train/train.csv\").put(Body=csv_buffer.getvalue())\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "pd.DataFrame(validation).to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3_resource.Object(bucket, \"data/validation/validation.csv\").put(Body=csv_buffer.getvalue())\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "pd.DataFrame(test).to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3_resource.Object(bucket, \"data/test/test.csv\").put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28bb1c-0c55-45fd-8526-2b34f3abf27d",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Run this code to train, tune, and find the best candidate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59d579-6e65-4c9e-8fe5-1890f47408fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner\n",
    ")\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "\n",
    "# Training and Validation Input for SageMaker Training job\n",
    "s3_input_train = TrainingInput(\n",
    "    s3_data=f\"s3://{bucket}/data/train/\",content_type=\"csv\")\n",
    "s3_input_validation = TrainingInput(\n",
    "    s3_data=f\"s3://{bucket}/data/validation/\",content_type=\"csv\")\n",
    "\n",
    "# Hyperparameter used\n",
    "fixed_hyperparameters = {\n",
    "    \"eval_metric\":\"auc\",\n",
    "    \"objective\":\"binary:logistic\",\n",
    "    \"num_round\":\"100\",\n",
    "    \"rate_drop\":\"0.3\",\n",
    "    \"tweedie_variance_power\":\"1.4\"\n",
    "}\n",
    "\n",
    "# Use the built-in SageMaker algorithm\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\",region,\"0.90-2\")\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    hyperparameters=fixed_hyperparameters,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    output_path=\"s3://{}/output\".format(bucket),\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "    \"min_child_weight\": ContinuousParameter(1, 10),\n",
    "    \"alpha\": ContinuousParameter(0, 2),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "}\n",
    "objective_metric_name = \"validation:auc\"\n",
    "tuner = HyperparameterTuner(\n",
    "estimator, objective_metric_name,\n",
    "hyperparameter_ranges,max_jobs=10,max_parallel_jobs=2)\n",
    "\n",
    "# Tune\n",
    "tuner.fit({\n",
    "    \"train\":s3_input_train,\n",
    "    \"validation\":s3_input_validation\n",
    "    },include_cls_metadata=False)\n",
    "\n",
    "## Explore the best model generated\n",
    "tuning_job_result = boto3.client(\"sagemaker\").describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name\n",
    ")\n",
    "\n",
    "job_count = tuning_job_result[\"TrainingJobStatusCounters\"][\"Completed\"]\n",
    "print(\"%d training jobs have completed\" %job_count)\n",
    "## 10 training jobs have completed\n",
    "\n",
    "## Get the best training job\n",
    "\n",
    "from pprint import pprint\n",
    "if tuning_job_result.get(\"BestTrainingJob\",None):\n",
    "    print(\"Best Model found so far:\")\n",
    "    pprint(tuning_job_result[\"BestTrainingJob\"])\n",
    "else:\n",
    "    print(\"No training jobs have reported results yet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
