{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "461b20a7-74df-4036-bf1a-0fe6b80cd371",
   "metadata": {},
   "source": [
    "## Step 1: Data preprocessing\n",
    "\n",
    "Run this code to preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd59705-5320-4d07-9ec1-9989b22ccb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset\n",
    "    \"\"\"\n",
    "    # Convert to datetime columns\n",
    "    df[\"firstorder\"] = pd.to_datetime(df[\"firstorder\"], errors=\"coerce\")\n",
    "    df[\"lastorder\"] = pd.to_datetime(df[\"lastorder\"], errors=\"coerce\")\n",
    "    # Drop Rows with null values\n",
    "    df = df.dropna()\n",
    "    # Create column which gives the days between the first and last orders\n",
    "    df[\"first_last_days_diff\"] = (df[\"lastorder\"] - df[\"firstorder\"]).dt.days\n",
    "    # Create column which gives the days between creation and first order\n",
    "    df[\"created\"] = pd.to_datetime(df[\"created\"])\n",
    "    df[\"created_first_days_diff\"] = (df[\"created\"] - df[\"firstorder\"]).dt.days\n",
    "    # Drop unused columns\n",
    "    unused_columns = [\"custid\", \"created\", \"firstorder\", \"lastorder\"]\n",
    "    df.drop(unused_columns, axis=1, inplace=True)\n",
    "    # Apply one hot encoding on categorical columns\n",
    "    cat_columns = [\"favday\", \"city\"]\n",
    "    df = pd.get_dummies(df, prefix=cat_columns, columns=cat_columns, dtype=int)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Define the S3 bucket and file key\n",
    "bucket = \"churn-prediction-sagemaker-demo\"\n",
    "file_key = \"data/storedata_total.csv\"\n",
    "# Create an S3 client\n",
    "s3_client = boto3.client(\"s3\")\n",
    "# Get the object from S3\n",
    "obj = s3_client.get_object(Bucket=bucket, Key=file_key)\n",
    "# Read the object content and load it into a pandas DataFrame\n",
    "df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "# Preprocess the dataset\n",
    "storedata = preprocess_data(df)\n",
    "storedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e567604c-0441-4987-b9c7-34c44573f34f",
   "metadata": {},
   "source": [
    "## Step 2: Data loading\n",
    "\n",
    "Run this code to split the data and upload the splits to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c469b-49a3-401a-988b-84e55e5219b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "def split_dataset(df):\n",
    "    y = df.pop(\"retained\")\n",
    "    X_pre = df\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "    feature_names = list(X_pre.columns)\n",
    "    X = np.concatenate((y_pre, X_pre), axis=1)\n",
    "    np.random.shuffle(X)\n",
    "    train, validation, test = np.split(X, [int(.7*len(X)), int(.85*len(X))])\n",
    "    return feature_names, train, validation, test\n",
    "\n",
    "\n",
    "# Copy the dataset for easier debugging\n",
    "df = storedata.copy()\n",
    "# Split dataset\n",
    "feature_names, train, validation, test = split_dataset(df)\n",
    "\n",
    "# Save datasets in Amazon S3\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "pd.DataFrame(train).to_csv(csv_buffer, header=False, index=False)\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3_resource.Object(bucket, \"data/train/train.csv\").put(Body=csv_buffer.getvalue())\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "pd.DataFrame(validation).to_csv(csv_buffer, header=False, index=False)\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3_resource.Object(bucket, \"data/validation/validation.csv\").put(Body=csv_buffer.getvalue())\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "pd.DataFrame(test).to_csv(csv_buffer, header=False, index=False)\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3_resource.Object(bucket, \"data/test/test.csv\").put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28bb1c-0c55-45fd-8526-2b34f3abf27d",
   "metadata": {},
   "source": [
    "## Step 3: Hyperparameter optimization\n",
    "\n",
    "Run this code to train, tune, and find the best candidate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59d579-6e65-4c9e-8fe5-1890f47408fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner\n",
    ")\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "\n",
    "# Training and Validation Input for SageMaker Training job\n",
    "s3_input_train = TrainingInput(\n",
    "    s3_data=f\"s3://{bucket}/data/train/\", content_type=\"csv\")\n",
    "s3_input_validation = TrainingInput(\n",
    "    s3_data=f\"s3://{bucket}/data/validation/\", content_type=\"csv\")\n",
    "\n",
    "# Hyperparameter used\n",
    "fixed_hyperparameters = {\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"100\",\n",
    "    \"rate_drop\": \"0.3\",\n",
    "    \"tweedie_variance_power\": \"1.4\"\n",
    "}\n",
    "\n",
    "# Use the built-in SageMaker algorithm\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"0.90-2\")\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    hyperparameters=fixed_hyperparameters,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    output_path=\"s3://{}/output\".format(bucket),\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "    \"min_child_weight\": ContinuousParameter(1, 10),\n",
    "    \"alpha\": ContinuousParameter(0, 2),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "}\n",
    "objective_metric_name = \"validation:auc\"\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator, objective_metric_name,\n",
    "    hyperparameter_ranges, max_jobs=10, max_parallel_jobs=2)\n",
    "\n",
    "# Tune\n",
    "tuner.fit({\n",
    "    \"train\": s3_input_train,\n",
    "    \"validation\": s3_input_validation\n",
    "    }, include_cls_metadata=False)\n",
    "\n",
    "# Explore the best model generated\n",
    "tuning_job_result = boto3.client(\"sagemaker\").describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name\n",
    ")\n",
    "\n",
    "job_count = tuning_job_result[\"TrainingJobStatusCounters\"][\"Completed\"]\n",
    "print(\"%d training jobs have completed\" %job_count)\n",
    "# 10 training jobs have completed\n",
    "\n",
    "# Get the best training job\n",
    "\n",
    "from pprint import pprint\n",
    "if tuning_job_result.get(\"BestTrainingJob\", None):\n",
    "    print(\"Best Model found so far:\")\n",
    "    pprint(tuning_job_result[\"BestTrainingJob\"])\n",
    "else:\n",
    "    print(\"No training jobs have reported results yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c08920-1edd-472e-9c61-8e06de6986c9",
   "metadata": {},
   "source": [
    "## Step 4: Fit best model with SageMaker Debugger\n",
    "\n",
    "This fits the best model and attaches a SageMaker Debugger hook to configure SHAP for feature explainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9613907-6a34-4e61-9455-46aa23b52f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import DebuggerHookConfig, CollectionConfig\n",
    "from sagemaker.debugger import rule_configs, Rule\n",
    "\n",
    "best_hyperparameters = tuning_job_result[\"BestTrainingJob\"][\"TunedHyperParameters\"]\n",
    "hyperparameters = {**fixed_hyperparameters,**best_hyperparameters}\n",
    "save_interval = 5\n",
    "base_job_name = \"demo-smdebug-xgboost-churn-classification\"\n",
    "\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"0.90-2\")\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    base_job_name=base_job_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    output_path=\"s3://{}/output\".format(bucket),\n",
    "    sagemaker_session=sess,\n",
    "    hyperparameters=hyperparameters,\n",
    "    max_run=1800,\n",
    "    debugger_hook_config = DebuggerHookConfig(\n",
    "        s3_output_path=f\"s3://{bucket}/debugger/\",\n",
    "        collection_configs=[\n",
    "            CollectionConfig(\n",
    "                name=\"metrics\",\n",
    "                parameters={\n",
    "                    \"save_interval\": \"5\"\n",
    "                }),\n",
    "            CollectionConfig(\n",
    "                name=\"feature_importance\", parameters={\"save_interval\": \"5\"}\n",
    "            ),\n",
    "            CollectionConfig(name=\"full_shap\", parameters={\"save_interval\": \"5\"}),\n",
    "            CollectionConfig(name=\"average_shap\", parameters={\"save_interval\": \"5\"}),\n",
    "        ]\n",
    "    ),\n",
    "    rules=[\n",
    "        Rule.sagemaker(\n",
    "            rule_configs.loss_not_decreasing(),\n",
    "            rule_parameters={\n",
    "                \"collection_names\": \"metrics\",\n",
    "                \"num_steps\": \"10\",\n",
    "            },\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "estimator.fit(\n",
    "        {\"train\":s3_input_train,\"validation\":s3_input_validation}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa944b3-1e9c-4f05-8f5a-2b60a5014abf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T10:39:19.750708Z",
     "iopub.status.busy": "2025-05-02T10:39:19.750010Z",
     "iopub.status.idle": "2025-05-02T10:39:19.753934Z",
     "shell.execute_reply": "2025-05-02T10:39:19.753234Z",
     "shell.execute_reply.started": "2025-05-02T10:39:19.750677Z"
    }
   },
   "source": [
    "## Step 5: Analyze Debugger output\n",
    "\n",
    "Here we analyze debugger output with some visualizations.\n",
    "\n",
    "First we have to install [sagemaker-debugger](https://github.com/awslabs/sagemaker-debugger) (SMdebug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1dd976-a42a-46c3-aafc-074dea2395f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install smdebug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2a4b3a-7644-416a-ab6e-30832b52b570",
   "metadata": {},
   "source": [
    "Then we have to collect the debugger output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee89d2-6497-4cd0-a5c3-c32a881dc138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.trials import create_trial\n",
    "\n",
    "s3_output_path = estimator.latest_job_debugger_artifacts_path()\n",
    "trial = create_trial(s3_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858525ec-4401-45dd-865c-16dbf1441884",
   "metadata": {},
   "source": [
    "### AUC for training and validation sets during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded45a4-eb96-4ea3-b5bf-95138e0f1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_data(trial, tname):\n",
    "    \"\"\"\n",
    "    For the given tensor name, walks though all the iterations\n",
    "    for which you have data and fetches the values.\n",
    "    Returns the set of steps and the values.\n",
    "    \"\"\"\n",
    "    tensor = trial.tensor(tname)\n",
    "    steps = tensor.steps()\n",
    "    vals = [tensor.value(s) for s in steps]\n",
    "    return steps, vals\n",
    "\n",
    "\n",
    "def match_tensor_name_with_feature_name(tensor_name, feature_names=feature_names):\n",
    "    feature_tag = tensor_name.split(\"/\")\n",
    "    for ifeat, feature_name in enumerate(feature_names):\n",
    "        if feature_tag[-1] == \"f{}\".format(str(ifeat)):\n",
    "            return feature_name\n",
    "    return tensor_name\n",
    "\n",
    "def get_data(trial, tname):\n",
    "    \"\"\"\n",
    "    For the given tensor name, walks though all the iterations\n",
    "    for which you have data and fetches the values.\n",
    "    Returns the set of steps and the values.\n",
    "    \"\"\"\n",
    "    tensor = trial.tensor(tname)\n",
    "    steps = tensor.steps()\n",
    "    vals = [tensor.value(s) for s in steps]\n",
    "    return steps, vals\n",
    "\n",
    "\n",
    "def match_tensor_name_with_feature_name(tensor_name, feature_names=feature_names):\n",
    "    feature_tag = tensor_name.split(\"/\")\n",
    "    for ifeat, feature_name in enumerate(feature_names):\n",
    "        if feature_tag[-1] == \"f{}\".format(str(ifeat)):\n",
    "            return feature_name\n",
    "    return tensor_name\n",
    "\n",
    "\n",
    "def plot_collection(trial, collection_name, regex=\".*\", max_tensors = 100, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Takes a `trial` and a collection name, and\n",
    "    plots all tensors that match the given regex.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    tensors = trial.collection(collection_name).tensor_names\n",
    "    matched_tensors = [t for t in tensors if re.match(regex, t)]\n",
    "    print(matched_tensors)\n",
    "    for tensor_name in islice(matched_tensors, max_tensors):\n",
    "        steps, data = get_data(trial, tensor_name)\n",
    "        ax.plot(steps, data, label=match_tensor_name_with_feature_name(tensor_name))\n",
    "\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "plot_collection(trial, \"metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b1f741-c691-4b38-a520-825b3cfe39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309c02d9-4695-431c-897d-db6baa8b88dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_collection_all_features(trial, collection_name, regex=\".*\", max_tensors = 100, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Redefine plot_collection() to get all tensors for\n",
    "    feature importance with trial.tensor_names()\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    # This is what we changed \n",
    "    tensors = trial.tensor_names()\n",
    "    matched_tensors = [t for t in tensors if re.match(regex, t)]\n",
    "    print(matched_tensors)\n",
    "    for tensor_name in islice(matched_tensors, max_tensors):\n",
    "        steps, data = get_data(trial, tensor_name)\n",
    "        ax.plot(steps, data, label=match_tensor_name_with_feature_name(tensor_name))\n",
    "\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "def plot_feature_importance(trial, importance_type=\"weight\"):\n",
    "    SUPPORTED_IMPORTANCE_TYPES = [\"weight\", \"gain\", \"cover\", \"total_gain\", \"total_cover\"]\n",
    "    if importance_type not in SUPPORTED_IMPORTANCE_TYPES:\n",
    "        raise ValueError(f\"{importance_type} is not one of the supported importance types.\")\n",
    "    plot_collection_all_features(trial, \"feature_importance\", regex=f\"feature_importance/{importance_type}/.*\")\n",
    "\n",
    "\n",
    "plot_feature_importance(trial, importance_type=\"cover\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6460f7-cf48-4052-911a-a2dc63fee6d0",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f167229-3905-4531-adf1-a9c8ec419ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap\n",
    "\n",
    "import shap\n",
    "\n",
    "shap_values = trial.tensor(\"full_shap/f0\").value(trial.last_complete_step)\n",
    "shap_no_base = shap_values[:, :-1]\n",
    "train_shap = pd.DataFrame(train[:, 1:], columns=feature_names)\n",
    "shap.summary_plot(shap_no_base, train_shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2385a7c2-5895-4041-a71c-c2af376cb3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.tensor_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca612f7-d400-4842-8cdc-4dbf61ddef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MAX_PLOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022bbeb5-cd30-4225-96a8-1778a5b5f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079d0a97-551a-4679-bc88-f34af40e03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.collection(\"feature_importance\").tensor_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f56415c-4b3c-4977-8c05-f8ddd7538641",
   "metadata": {},
   "outputs": [],
   "source": [
    "?trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91888a3-ec1d-4eae-b4b9-90de56dfcc33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
